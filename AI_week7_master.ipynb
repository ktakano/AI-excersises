{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5FRXS85qAZmY",
        "9aIsWInjUrU2",
        "QANFdYqQW4ri",
        "hiV4yWRxbBtk",
        "Iib5GxyjHmAQ",
        "8Cf5GuUsQ9ey",
        "WILtPxQYYPHF",
        "AQniugEjdyrH",
        "UyYK--AdBcrT",
        "Pzhr6xuYv5u4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJOyLI1m_2PI"
      },
      "source": [
        "# AI演習 第7回\n",
        "### ディープラーニングによる自然言語処理 (1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[実行環境]<br>\n",
        "Colaboratoryの実行環境は更新されるので確認しておきます。<br>\n",
        "Python: 3.10.12<br>\n",
        "\n",
        "ランタイムのタイプは、GPUを指定するように注意してください。"
      ],
      "metadata": {
        "id": "oCsUej-OG9vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2EtscWwG6WF",
        "outputId": "864ab345-2d19-4a96-b692-7d7cbf77831a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FRXS85qAZmY"
      },
      "source": [
        "### 自然言語処理とは\n",
        "* 日本語、英語、タイ語など、日常の会話などで使用する言語を自然言語と言います。\n",
        "自然言語処理とは、自然言語を対象として単語や句の抽出や、係り受けの解析といった処理を経て、コンピュータが機械翻訳、自動要約、文章分類、文脈理解、会話生成などを行えるようにするための処理です。\n",
        "\n",
        "* 自然言語処理の応用例\n",
        " * 文書分類\n",
        " * 検索\n",
        " * 機械翻訳\n",
        " * 文書要約\n",
        " * 質問応答\n",
        " * 対話\n",
        " * 品詞タグ付け\n",
        " * 単語分割\n",
        " * 語義曖昧性解消\n",
        " * 固有表現抽出\n",
        " * 構文解析\n",
        " * 述語項構造認識\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aIsWInjUrU2"
      },
      "source": [
        "### 自然言語処理とディープラーニング\n",
        "* トピックモデルによる文書中の単語の出現確率の推定や隠れマルコフモデルによる品詞推定などが行われてきました。\n",
        "\n",
        "* 2013年にword2vecのような単語の分散表現をニューラルネットワークで学習する方式が考案され、さらにRNNやLSTMが自然言語処理に適用されるとともに、機械翻訳、対話文生成、自動要約、画像説明文の生成などの応用が広がっていきました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QANFdYqQW4ri"
      },
      "source": [
        "### 言語モデルと文脈\n",
        "* 単語が文書中に出現する過程を確率過程と見なし、単語がある位置に出現する確率を計算するモデルを言語モデルといいます。また言語モデルにおいて，ある単語の出現確率を計算する際に用いる周囲の単語を文脈といいます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiV4yWRxbBtk"
      },
      "source": [
        "### 自然言語処理の流れ\n",
        "\n",
        "1.   データセットの準備\n",
        "2.   前処理\n",
        "3.   単語の数値化\n",
        "4.   データセットの学習 = 応用モデルの構築\n",
        "5.   応用モデルによる分類や回帰\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iib5GxyjHmAQ"
      },
      "source": [
        "### 前処理\n",
        "* 形態素解析、n-gram分割、ストップワードなどの処理をして、文章を解析プログラムが処理しやすい形式に加工することです。\n",
        "\n",
        " * 表記の統一\n",
        " * 小文字化と大文字化\n",
        " * 単語置換\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cf5GuUsQ9ey"
      },
      "source": [
        "### 形態素解析の実行例\n",
        "\n",
        "形態素解析ライブラリjanomeを利用した形態素解析の実行例です。最初にjanomeをインストールしています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3tuDZHndDQI"
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6bvuBeqcmY_"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "text = '私は空を見て、あなたは鏡をのぞき込む。'\n",
        "\n",
        "jt = Tokenizer()\n",
        "for token in jt.tokenize(text):\n",
        "  print(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dQrgL0PdZVH"
      },
      "source": [
        "for token in jt.tokenize(text, wakati=True):\n",
        "  print(token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-1 確認例題\n",
        "次の文章を形態素解析し、区切られた単語ごとに表示してみましょう。\n",
        "\n",
        "文章: 鏡の中で、あなたは鳥が青空を飛んで行くのを見た。"
      ],
      "metadata": {
        "id": "HZ4Q8MDc5TSy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RovHcmNZeK6V"
      },
      "source": [
        "### 単語の数値化\n",
        "* 単語間の関連性などを定量的に計算できるように、単語を数値化します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BQqy1otqT41"
      },
      "source": [
        "text = 'I look at sky and you look in mirror.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6R6ykJgmKSj"
      },
      "source": [
        "text = text.lower() # 小文字にする\n",
        "text = text.replace('.', ' .') # ピリオドを分割\n",
        "words = text.split(' ') # 空白を区切りにして単語を分割"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "EFisl1ISc4Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVbMTEs7311x"
      },
      "source": [
        "print (words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwn6uLqWlsK7"
      },
      "source": [
        "def word2id(words):\n",
        "\n",
        "  word_to_id = {}\n",
        "\n",
        "  for word in words:\n",
        "    if word not in word_to_id:\n",
        "      new_id = len(word_to_id)\n",
        "      word_to_id[word] = new_id\n",
        "\n",
        "  return word_to_id\n",
        "\n",
        "def id2word(word_to_id):\n",
        "  id_to_word = {}\n",
        "  for word, id in word_to_id.items():\n",
        "    id_to_word[id] = word\n",
        "\n",
        "  return id_to_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b5HAFR4nyKF"
      },
      "source": [
        "# 単語の数値化\n",
        "word_to_id = word2id(words)\n",
        "print(word_to_id)\n",
        "\n",
        "# 数値からの逆引き\n",
        "id_to_word = id2word(word_to_id)\n",
        "print(id_to_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_to_id['look'])\n",
        "print(id_to_word[7])"
      ],
      "metadata": {
        "id": "FGulQgh_fCJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-2 確認例題\n",
        "次の文章について、word2id()を使用して、単語をidに変換してみましょう。また、id2word()を使用して、idから対応する単語を抽出してみましょう。\n",
        "\n",
        "In the mirror, you saw a bird flying across the blue sky."
      ],
      "metadata": {
        "id": "Z6EczzQV8XZA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR9D50p0I1dw"
      },
      "source": [
        "### One-hotベクトル表現\n",
        "要素の値が0と1からなり、かつ1か所だけが1のベクトルです。自然言語を処理するニューラルネットワークでは、one-hotベクトルで表現されたものを入力とすることが多いです。\n",
        "\n",
        "'I look at sky and you look in mirror.'<br>\n",
        "を上の例のように分解した場合、数値を要素番号とすると各単語のone-hotベクトルを下記のように生成することができます。\n",
        "\n",
        "i: [1, 0, 0, 0, 0, 0, 0, 0, 0] <br>\n",
        "look: [0, 1, 0, 0, 0, 0, 0, 0, 0] <br>\n",
        "at: [0, 0, 1, 0, 0, 0, 0, 0, 0] <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pczPYaF5k_6m"
      },
      "source": [
        "# 単語-数値リストのデータを、one-hot形式に変換\n",
        "def make_one_hot(corpus):\n",
        "    N = corpus.shape[0]\n",
        "    dim =len(word_to_id)\n",
        "\n",
        "    one_hot = np.zeros((N, dim), dtype=np.int32)\n",
        "    for idx, word_id in enumerate(corpus):\n",
        "        one_hot[idx, word_id] = 1\n",
        "\n",
        "    return one_hot\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfnimEn_TfZ9"
      },
      "source": [
        "import numpy as np\n",
        "print(words)\n",
        "corpus = [word_to_id[word] for word in words]\n",
        "corpus = np.array(corpus)\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "make_one_hot(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-3 確認例題\n",
        "次の文章について、one-hotベクトル表現に変換してみましょう。\n",
        "\n",
        "In the mirror, you saw a bird flying across the blue sky."
      ],
      "metadata": {
        "id": "P6mhwVKf9RmV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILtPxQYYPHF"
      },
      "source": [
        "### 単語の分散表現（埋め込み表現）\n",
        "* 単語をベクトルで表現したものです。\n",
        "* 単語分散表現を得るためのニューラルネットワークを用いた手法として，下記のものがあります。\n",
        "\n",
        " * Word2Vec\n",
        " * GloVe\n",
        " * fastText\n",
        "\n",
        "* 通常の深層学習を使用して学習させ、単語分散表現を得ることもできます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQniugEjdyrH"
      },
      "source": [
        "### Word2Vec\n",
        "* 単語の分散表現(埋め込み)を生成する手法\n",
        "\n",
        "* CBOWモデルとskip gramモデルを使用\n",
        "* 2013年にTomas Mikolovらが考案\n",
        "* 分布仮説（単語の意味は周囲の単語によって形成されるという仮説）に基づいて、単語の意味表現を学習する。\n",
        "* 2層のニューラルネットワークで学習する。隠れ層の重みが単語の分散表現となる。\n",
        "\n",
        "* 生成された単語の分散表現は、各単語の意味を表したベクトル行列となっており，単語間の距離を計算できる。\n",
        "\n",
        " * vector('Paris') - vector('France') + vector('Italy’) = vector(‘Rome’) <br>\n",
        " * vector('king') - vector('man') + vector('woman’) = vector('queen')\n",
        "\n",
        "<br>\n",
        "\n",
        "#### CBOWモデル\n",
        "\n",
        "* 複数の単語を文脈として、一つの単語を予測する。文脈単語の順序は問わない。\n",
        "<br>\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1zGpPVAQ7hBmkjEkeFMNa0p5vHXv192hq' width='60%'>\n",
        "\n",
        "#### Skip-gramモデル\n",
        "\n",
        "* 一つの単語を使用してして、複数の単語を文脈として予測する。文脈単語は、入力単語の近さに応じて重みを決める。\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=18ipD4BXzaNA9tnggGE8yZTGA0mD70rSB' width='60%'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-4 確認例題\n",
        "Word2VecのCBOWモデルとskip-gramモデルについて、それぞれ単語の分散行列を作成するための学習方法を説明してみましょう。"
      ],
      "metadata": {
        "id": "0O9Wml3-IzUH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwVwumo2ayxr"
      },
      "source": [
        "### 単語の分散表現行列の作成\n",
        "\n",
        "下記のサイトで提供されるコーパスを用いて、単語の分散表現行列を作成します。\n",
        "\n",
        "(英語) http://mattmahoney.net/dc/textdata.html <br>\n",
        "(日本語) https://github.com/Hironsan/ja.text8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwJauWPr6x8n"
      },
      "source": [
        "!wget http://mattmahoney.net/dc/text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM2rcdUSaHG4"
      },
      "source": [
        "!unzip text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKVoWs-bbant"
      },
      "source": [
        "import logging\n",
        "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "sentences = Text8Corpus('text8')\n",
        "model = Word2Vec(sentences, vector_size=100)\n",
        "\n",
        "model.save('model.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT5c1-GibtnA"
      },
      "source": [
        "model = Word2Vec.load('model.bin')\n",
        "\n",
        "# dogの分散表現ベクトル\n",
        "print(model.wv['dog'])\n",
        "print(model.wv['dog'].shape)\n",
        "# carと意味が似ている単語を抽出\n",
        "model.wv.most_similar(['car'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の説明で用いた下記の式が成り立つかを確認してみます。\n",
        "\n",
        "vector('Paris') - vector('France') + vector('Italy’) = vector(‘Rome’)\n"
      ],
      "metadata": {
        "id": "pmvqIEnU99yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = model.wv['paris'] - model.wv['france'] + model.wv['italy']\n",
        "\n",
        "model.wv.most_similar(vector)"
      ],
      "metadata": {
        "id": "Ft2kzadN-R1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-5 確認例題\n",
        "下記の式が成り立つかを試してみましょう。\n",
        "\n",
        "vector('king') - vector('man') + vector('woman’) = vector('queen')"
      ],
      "metadata": {
        "id": "2U_A9unJ_zqb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3XGfFKib7_P"
      },
      "source": [
        "#### 1-6 確認例題\n",
        "日本語コーパス(ja.text8.zip)をダウンロードし、同様の処理をしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIZJt8tGEafr"
      },
      "source": [
        "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip\n",
        "!unzip ja.text8.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P14GRXFUTAqK"
      },
      "source": [
        "import logging\n",
        "from gensim.models.word2vec import Word2Vec, Text8Corpus\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "sentences = Text8Corpus('ja.text8')\n",
        "model_ja = Word2Vec(sentences, vector_size=100)\n",
        "\n",
        "model_ja.save('model.ja.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7pkHfROBj6H"
      },
      "source": [
        "model_ja = Word2Vec.load('model.ja.bin')\n",
        "\n",
        "print(model_ja.wv['犬'])\n",
        "print(model_ja.wv['犬'].shape)\n",
        "model_ja.wv.most_similar(['車'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYK--AdBcrT"
      },
      "source": [
        "### ベクトル空間モデル\n",
        "\n",
        "\n",
        "*   情報検索を行うためのアルゴリズム\n",
        "*   1970年頃から研究が始まる\n",
        " * Salton先生等のSMARTシステムが有名\n",
        "\n",
        "* ベクトル空間上に検索対象データ，検索語をそれぞれベクトルで表現して配置する．\n",
        "* 検索対象データ，検索語の類似度をベクトル計算（コサイン，内積，距離等）により求める。\n",
        "<img src='https://drive.google.com/uc?export=view&id=14u9m4dFPDTmMUa4sMLMErm6CwViEiqpX' width='30%'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzhr6xuYv5u4"
      },
      "source": [
        "### 文書単語行列\n",
        "* 文書(d1～dm)中に出現する単語集合(t1～tn)によって，特徴付けられた行列\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1y4V0uPCEPcm0JH_v0-NHWtz0SW4_bGkc' width='30%'>\n",
        "\n",
        "<br>\n",
        "\n",
        "* 文書単語行列の例:\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1tEOkD8_BnWLuZxf9iiD1zlVCl8Zn7DAv' width='50%'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prbhDcajopBJ"
      },
      "source": [
        "### コサイン計算\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1a226Fn74_d-un3DM-Z_d41yWixYQiYFy' width='60%'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjnKL_6Votxg"
      },
      "source": [
        "分散表現ベクトルのコサイン尺度を計算する関数cosine_sim()を作成し、「car」と「truck」の分散表現ベクトルのコサイン尺度を計算してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00ojpNAlmJTq"
      },
      "source": [
        "# コサインを計算する\n",
        "def cosine_sim(w1, w2):\n",
        "  cosine_value = np.dot(model.wv[w1], model.wv[w2]) / (np.linalg.norm(model.wv[w1]) * np.linalg.norm(model.wv[w2]))\n",
        "\n",
        "  return cosine_value\n",
        "\n",
        "print (cosine_sim('car','truck'))\n",
        "\n",
        "# ('truck', 0.7117820978164673)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il7CwmCEpzxf"
      },
      "source": [
        "コサイン尺度は0.7151368と出力され、先ほどの「model.wv.most_similar(['car'])」の結果と同じ値が算出されていることが確認できます。\n",
        "\n",
        "[('driver', 0.7645907402038574), <br>\n",
        " ('cars', 0.7256535291671753), <br>\n",
        " ('motorcycle', 0.7231885194778442), <br>\n",
        " ('taxi', 0.7163602113723755), <br>\n",
        " ('truck', 0.7151368856430054), <br>\n",
        " ('vehicle', 0.6943105459213257), <br>\n",
        "...]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMPoB4b-DmNA"
      },
      "source": [
        "#### 1-7 確認例題\n",
        "コサイン計算プログラムを利用して、「車」と「電車」の分散表現ベクトルのコサイン尺度を計算してみましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-8 確認例題\n",
        "(1)コサイン計算プログラムを利用して、下記式の左辺のベクトルと右辺のベクトルのコサイン尺度を計算しましょう。\n",
        "\n",
        "(2)計算したコサイン尺度に基づいて、下記式が成立するかを考察しましょう。\n",
        "\n",
        "vector('Paris') - vector('France') + vector('Italy’) = vector(‘Rome’)\n",
        "\n",
        "vector('king') - vector('man') + vector('woman’) = vector('queen')"
      ],
      "metadata": {
        "id": "t_TJCLUwBI4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文書ベクトルの作成\n",
        "\n",
        "文書中の単語をベクトル化し、それらの単語ベクトルを足すことで、文章ベクトルを作成します。\n",
        "\n",
        "文書1: apple, banana, grape, pear, strawberry\n",
        "\n",
        "文書2: grape, pear, mango, melon, watermelon\n"
      ],
      "metadata": {
        "id": "-fkoPFUOJfGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec1 = model.wv['apple'] + model.wv['banana'] + model.wv['grape'] + model.wv['pear'] + model.wv['strawberry']\n",
        "\n",
        "print (doc_vec1)"
      ],
      "metadata": {
        "id": "yyw5TIVaJcU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_vec2 = model.wv['grape'] + model.wv['pear'] + model.wv['mango'] + model.wv['melon'] + model.wv['watermelon']\n",
        "\n",
        "print (doc_vec2)"
      ],
      "metadata": {
        "id": "_k9RcwJyK-l4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文書1と文書2のコサイン尺度を算出します。"
      ],
      "metadata": {
        "id": "pJNcynqiLKl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " cosine_value = np.dot(doc_vec1, doc_vec2) / (np.linalg.norm(doc_vec1) * np.linalg.norm(doc_vec2))\n",
        "\n",
        " print(cosine_value)"
      ],
      "metadata": {
        "id": "Um32S5NVLOPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-9 確認例題\n",
        "コサイン尺度の関数を修正し、2つのベクトルを入力して、コサイン尺度を返す関数cosine_sim_vec(v1, v2)を作成してみましょう。"
      ],
      "metadata": {
        "id": "7nGKQu1FMNX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1-10 確認例題\n",
        "さらに、次の文書ベクトルを作成します。\n",
        "\n",
        "(1) 文書1と文書4のコサイン尺度、および、文書4と文書5のコサイン尺度をそれぞれ計算しましょう。\n",
        "\n",
        "(2) 文書の内容とコサイン尺度の値から、文書1と文書4の類似度(どれくらい似ているかを示す度合い)、および、文書4と文書5の類似度について説明してみましょう。\n",
        "\n",
        "文書3: peach, melon, banana, strawberry, orange\n",
        "\n",
        "文書4: mathematics, physics, french, geography, chemistry\n",
        "\n",
        "文書5: mathematics, chemistry, japanese, geography, english"
      ],
      "metadata": {
        "id": "dPdfaj7_JXgc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DnM6Zkwy33I"
      },
      "source": [
        "### 参考文献\n",
        "* https://code.google.com/archive/p/word2vec/\n",
        "* Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, 2013.\n",
        "* François Chollet, Deep Learning with Python\n",
        "* 斎藤 康毅，ゼロから作るDeep Learning 2 ―自然言語処理編\n",
        "* 中山光樹，機械学習・深層学習による自然言語処理入門 scikit-learnとTensorFlowを使った実践プログラミング"
      ]
    }
  ]
}